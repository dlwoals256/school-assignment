{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9836816a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os, math, json, random, time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da3797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Config\n",
    "CSV_TRAIN = \"index_train.csv\"\n",
    "CSV_VAL   = \"index_val.csv\"\n",
    "CSV_TEST  = \"index_test.csv\"\n",
    "\n",
    "T_LEN = 16            # window length (Number of f0 ... f15 in CSV)\n",
    "IMG_SIZE = 224        # Resolution of frame (e.g. 224 -> 224 * 224)\n",
    "BATCH_TRAIN = 8\n",
    "BATCH_VAL   = 8\n",
    "NUM_WORKERS = 4\n",
    "PIN_MEMORY  = torch.cuda.is_available()\n",
    "\n",
    "# Normalization ImageNet\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD  = (0.229, 0.224, 0.225)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b79561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2) Dataset\n",
    "class MultiTargetWindowDataset(Dataset):\n",
    "    \"\"\"\n",
    "    One row of index_*.csv = 16 of frame path + 4 regression target(y_ttc_inv, y_dmin_inv, y_tstar_norm, y_dist_inv)\n",
    "                      + 4 raw + 4 mask(0/1)\n",
    "\n",
    "    Return:\n",
    "      x: [T, 3, H, W] FloatTensor\n",
    "      y: [4] FloatTensor   (Sequence: y_ttc_inv, y_dmin_inv, y_tstar_norm, y_dist_inv)\n",
    "      m: [4] FloatTensor   (Valid mask of each target as 0/1)\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_path, t_len=16, image_size=224, augment=False):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.t_len = t_len\n",
    "        self.augment = augment\n",
    "\n",
    "        # Pre-cache\n",
    "        self.frame_cols = [f\"f{k}\" for k in range(self.t_len)]\n",
    "\n",
    "        base_trans = [\n",
    "            T.Resize((image_size, image_size)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "        ]\n",
    "        if augment:\n",
    "            self.transform = T.Compose([\n",
    "                T.Resize(int(image_size*1.1)),\n",
    "                T.RandomResizedCrop(image_size, scale=(0.75, 1.0)),\n",
    "                T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = T.Compose(base_trans)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _safe_load_rgb(self, path):\n",
    "        '''\n",
    "        디버깅용 이미지 예외처리\n",
    "        '''\n",
    "        try:\n",
    "            with Image.open(path) as im:\n",
    "                return im.convert(\"RGB\")\n",
    "        except Exception:\n",
    "            return Image.fromarray(np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Load frame stack\n",
    "        imgs = []\n",
    "        for col in self.frame_cols:\n",
    "            p = row[col]\n",
    "            img = self._safe_load_rgb(p)\n",
    "            img = self.transform(img)\n",
    "            imgs.append(img)\n",
    "        x = torch.stack(imgs, dim=0)   # [T, 3, H, W]\n",
    "\n",
    "        # Regression target\n",
    "        y = torch.tensor([\n",
    "            float(row[\"y_ttc_inv\"]),\n",
    "            float(row[\"y_dmin_inv\"]),\n",
    "            float(row[\"y_tstar_norm\"]),\n",
    "            float(row[\"y_dist_inv\"]),\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        # Mask(0/1, for hubor loss)\n",
    "        m = torch.tensor([\n",
    "            float(row[\"mask_ttc\"]),\n",
    "            float(row[\"mask_dmin\"]),\n",
    "            float(row[\"mask_tstar\"]),\n",
    "            float(row[\"mask_dist\"]),\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        return x, y, m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9e4ba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MultiTargetWindowDataset(CSV_TRAIN, t_len=T_LEN, image_size=IMG_SIZE, augment=True)\n",
    "val_ds   = MultiTargetWindowDataset(CSV_VAL,   t_len=T_LEN, image_size=IMG_SIZE, augment=False)\n",
    "test_ds   = MultiTargetWindowDataset(CSV_TEST,   t_len=T_LEN, image_size=IMG_SIZE, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c719cfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_TRAIN,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_dl = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_VAL,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_dl = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=BATCH_VAL,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc437bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([8, 16, 3, 224, 224]) y: torch.Size([8, 4]) mask: torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "xb, yb, mb = next(iter(train_dl))\n",
    "print(\"x:\", xb.shape, \"y:\", yb.shape, \"mask:\", mb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d01c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalTCN(nn.Module):\n",
    "    def __init__(self, c, ks=3, dilations=(1, 2, 4, 8), dropout=0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for d in dilations:\n",
    "            pad = (ks - 1) * d  # causal padding\n",
    "            layers += [\n",
    "                nn.Conv1d(c, c, ks, padding=pad, dilation=d),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "            ]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):  # x:[B,C,T]\n",
    "        y = self.net(x)\n",
    "        return y[..., -1]  # Return last step [B,C]\n",
    "\n",
    "class ResNetTemporalMultiHead(nn.Module):\n",
    "    def __init__(self, T=16, feat_dim=512, head_dim=256, temporal=\"avg\"):\n",
    "        super().__init__()\n",
    "        base = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone = nn.Sequential(*list(base.children())[:-1])   # [B,512,1,1]\n",
    "        self.temporal = temporal\n",
    "        self.T = T\n",
    "\n",
    "        # 실험할 때 빠르게 전환 위해 조건문으로 구성\n",
    "        if temporal == \"avg\":\n",
    "            self.temporal_pool = nn.Identity()   # 나중에 mean으로 처리\n",
    "            temporal_out = feat_dim\n",
    "        elif temporal == \"tcn\":\n",
    "            self.temporal_pool = CausalTCN(feat_dim)\n",
    "            temporal_out = feat_dim\n",
    "        elif temporal == \"gru\":\n",
    "            self.gru = nn.GRU(input_size=feat_dim, hidden_size=feat_dim, batch_first=True, bidirectional=False)\n",
    "            temporal_out = feat_dim\n",
    "        else:\n",
    "            raise ValueError(\"temporal must be one of ['avg','tcn','gru']\")\n",
    "\n",
    "        def head():\n",
    "            return nn.Sequential(nn.Linear(temporal_out, head_dim), nn.ReLU(), nn.Linear(head_dim, 1))\n",
    "\n",
    "        self.head_ttc   = head()\n",
    "        self.head_dmin  = head()\n",
    "        self.head_tstar = head()\n",
    "        self.head_dist  = head()\n",
    "\n",
    "        '''\n",
    "        Explain:\n",
    "            Uncertainty parameter(log-sigma)\n",
    "            This learnable weights automatically learns\n",
    "            weight of multi-task weighted sum\n",
    "        '''\n",
    "        self.log_sigma = nn.ParameterDict({\n",
    "            \"ttc\":   nn.Parameter(torch.tensor(0.0)),\n",
    "            \"dmin\":  nn.Parameter(torch.tensor(0.0)),\n",
    "            \"tstar\": nn.Parameter(torch.tensor(0.0)),\n",
    "            \"dist\":  nn.Parameter(torch.tensor(0.0)),\n",
    "        })\n",
    "\n",
    "    def forward(self, x):  # x:[B,T,3,H,W]\n",
    "        B, T = x.shape[:2]\n",
    "        x = x.view(B*T, 3, x.size(3), x.size(4))\n",
    "        f = self.backbone(x).flatten(1)          # [B*T,512]\n",
    "        f = f.view(B, T, -1)                      # [B,T,512]\n",
    "\n",
    "        if self.temporal == \"avg\":\n",
    "            g = f.mean(dim=1)                     # [B,512]\n",
    "        elif self.temporal == \"tcn\":\n",
    "            g = f.transpose(1,2)                  # [B,512,T]\n",
    "            g = self.temporal_pool(g).squeeze(-1) # [B,512]\n",
    "        elif self.temporal == \"gru\":\n",
    "            _, h = self.gru(f)                    # h:[1,B,512]\n",
    "            g = h[-1]                             # [B,512]\n",
    "\n",
    "        out = {\n",
    "            \"ttc\":   self.head_ttc(g).squeeze(1),\n",
    "            \"dmin\":  self.head_dmin(g).squeeze(1),\n",
    "            \"tstar\": self.head_tstar(g).squeeze(1),\n",
    "            \"dist\":  self.head_dist(g).squeeze(1),\n",
    "        }\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22bc0a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNetTemporalMultiHead(\n",
      "  (backbone): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      "  (temporal_pool): CausalTCN(\n",
      "    (net): Sequential(\n",
      "      (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "      (4): ReLU()\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "      (7): ReLU()\n",
      "      (8): Dropout(p=0.1, inplace=False)\n",
      "      (9): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "      (10): ReLU()\n",
      "      (11): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (head_ttc): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (head_dmin): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (head_tstar): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (head_dist): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (log_sigma): ParameterDict(\n",
      "      (dist): Parameter containing: [torch.cuda.FloatTensor of size  (cuda:0)]\n",
      "      (dmin): Parameter containing: [torch.cuda.FloatTensor of size  (cuda:0)]\n",
      "      (tstar): Parameter containing: [torch.cuda.FloatTensor of size  (cuda:0)]\n",
      "      (ttc): Parameter containing: [torch.cuda.FloatTensor of size  (cuda:0)]\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ResNetTemporalMultiHead(T=T_LEN, temporal=\"tcn\").to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f852b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Util of unfreezing backbone ---\n",
    "def freeze_backbone(m, freeze=True):\n",
    "    for p in m.backbone.parameters():\n",
    "        p.requires_grad = (not freeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7446415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze for first 5 epoch → Only temporal/head learns\n",
    "FREEZE_EPOCHS = 5\n",
    "freeze_backbone(model, freeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5389ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze BatchNorm\n",
    "def freeze_bn(m):\n",
    "    import torch.nn as nn\n",
    "    if isinstance(m, (nn.BatchNorm2d, nn.SyncBatchNorm)):\n",
    "        m.eval()\n",
    "        for p in m.parameters():\n",
    "            p.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2c30b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision-specific unfreeze util\n",
    "def unfreeze_backbone_stage_by_index(backbone: nn.Sequential, stage_idx: int):\n",
    "    \"\"\"\n",
    "    Pop ResNet18 out from torchvision then wrap by nn.Sequential:\n",
    "    0 conv7x7, 1 bn1, 2 relu, 3 maxpool, 4 layer1, 5 layer2, 6 layer3, 7 layer4, 8 avgpool\n",
    "    \"\"\"\n",
    "    assert isinstance(backbone, nn.Sequential), \"backbone MUST be nn.Sequential\"\n",
    "    for p in backbone[stage_idx].parameters():\n",
    "        p.requires_grad_(True)\n",
    "    return f\"backbone[{stage_idx}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09151b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Loss of each target with mask and uncertainty weight ===\n",
    "def masked_huber(pred, target, mask, delta=1.0):\n",
    "    diff = pred - target\n",
    "    absd = diff.abs()\n",
    "    hub = torch.where(absd < delta, 0.5*diff*diff, delta*(absd - 0.5*delta))\n",
    "    w = (mask > 0).float()\n",
    "    return (hub * w).sum() / (w.sum() + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abd59a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multitask_loss(outputs, y, m, model):\n",
    "    # outputs: dict(\"ttc\",\"dmin\",\"tstar\",\"dist\") -> [B]\n",
    "    # y: [B,4]  (y_ttc_inv, y_dmin_inv, y_tstar_norm, y_dist_inv)\n",
    "    # m: [B,4]\n",
    "    L_ttc   = masked_huber(outputs[\"ttc\"],   y[:,0], m[:,0])\n",
    "    L_dmin  = masked_huber(outputs[\"dmin\"],  y[:,1], m[:,1])\n",
    "    L_tstar = masked_huber(outputs[\"tstar\"], y[:,2], m[:,2])\n",
    "    L_dist  = masked_huber(outputs[\"dist\"],  y[:,3], m[:,3])\n",
    "\n",
    "    # Weighted sum by uncertainty weight (Use log_sigma which be learned automatically)\n",
    "    def uw(name, L):\n",
    "        log_sigma = model.log_sigma[name]\n",
    "        return torch.exp(-2*log_sigma) * L + log_sigma\n",
    "    loss = uw(\"ttc\", L_ttc) + uw(\"dmin\", L_dmin) + uw(\"tstar\", L_tstar) + uw(\"dist\", L_dist)\n",
    "\n",
    "    logs = {\"L_ttc\": L_ttc.item(), \"L_dmin\": L_dmin.item(),\n",
    "            \"L_tstar\": L_tstar.item(), \"L_dist\": L_dist.item()}\n",
    "    return loss, logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bb1ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3) Optima/Scheduler ===\n",
    "# Update only head/temporal when it's freezed → Filtering parameter\n",
    "def optim_params(m):\n",
    "    return [p for p in m.parameters() if p.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "949398d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.AdamW(optim_params(model), lr=3e-4, weight_decay=1e-4)\n",
    "sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64cc3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4) Validation metric (MAE, physical units) ===\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve\n",
    "\n",
    "EPS = 1e-3\n",
    "H   = 4.0  # t* Horizon of used in normalization (index.csv 만들 때 썼던 거)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_epoch(dataloader):\n",
    "    model.eval()\n",
    "    n = 0\n",
    "    loss_sum = 0.0\n",
    "\n",
    "    # Init MAE(expressed by physical units)\n",
    "    mae_ttc = mae_dmin = mae_tstar = mae_dist = 0.0\n",
    "    cnt_ttc = cnt_dmin = cnt_tstar = cnt_dist = 0\n",
    "\n",
    "    # Buffer of sum for event-driven validation(whole validation)\n",
    "    yT_gt=[]; yT_pr=[]; mT=[]\n",
    "    yD_gt=[]; yD_pr=[]; mD=[]\n",
    "\n",
    "    for xb, yb, mb in dataloader:\n",
    "        xb, yb, mb = xb.to(device), yb.to(device), mb.to(device)\n",
    "        out = model(xb)\n",
    "        loss, logs = multitask_loss(out, yb, mb, model)\n",
    "        loss_sum += loss.item() * xb.size(0)\n",
    "        n += xb.size(0)\n",
    "\n",
    "        # ---- Inverse transform ----\n",
    "        # pred\n",
    "        y_ttc_inv_pred   = out[\"ttc\"]\n",
    "        y_dmin_inv_pred  = out[\"dmin\"]\n",
    "        y_tstar_norm_pred= out[\"tstar\"]\n",
    "        y_dist_inv_pred  = out[\"dist\"]\n",
    "        # tgt\n",
    "        y_ttc_inv_tgt    = yb[:,0]; y_dmin_inv_tgt = yb[:,1]\n",
    "        y_tstar_norm_tgt = yb[:,2]; y_dist_inv_tgt = yb[:,3]\n",
    "\n",
    "        # TTC [s]\n",
    "        if mb[:,0].sum() > 0:\n",
    "            ttc_pred = 1.0 / (y_ttc_inv_pred.clamp_min(1e-6) + EPS)\n",
    "            ttc_tgt  = 1.0 / (y_ttc_inv_tgt.clamp_min(1e-6)  + EPS)\n",
    "            diff = (ttc_pred - ttc_tgt).abs()\n",
    "            mae_ttc += (diff * mb[:,0]).sum().item()\n",
    "            cnt_ttc += mb[:,0].sum().item()\n",
    "\n",
    "        # dmin [m]\n",
    "        if mb[:,1].sum() > 0:\n",
    "            dmin_pred = 1.0 / (y_dmin_inv_pred.clamp_min(1e-6) + EPS)\n",
    "            dmin_tgt  = 1.0 / (y_dmin_inv_tgt.clamp_min(1e-6)  + EPS)\n",
    "            diff = (dmin_pred - dmin_tgt).abs()\n",
    "            mae_dmin += (diff * mb[:,1]).sum().item()\n",
    "            cnt_dmin += mb[:,1].sum().item()\n",
    "\n",
    "        # t* [s]\n",
    "        if mb[:,2].sum() > 0:\n",
    "            tstar_pred = y_tstar_norm_pred.clamp(0,1) * H\n",
    "            tstar_tgt  = y_tstar_norm_tgt.clamp(0,1)  * H\n",
    "            diff = (tstar_pred - tstar_tgt).abs()\n",
    "            mae_tstar += (diff * mb[:,2]).sum().item()\n",
    "            cnt_tstar += mb[:,2].sum().item()\n",
    "\n",
    "        # dist [m]\n",
    "        if mb[:,3].sum() > 0:\n",
    "            dist_pred = 1.0 / (y_dist_inv_pred.clamp_min(1e-6) + EPS)\n",
    "            dist_tgt  = 1.0 / (y_dist_inv_tgt.clamp_min(1e-6)  + EPS)\n",
    "            diff = (dist_pred - dist_tgt).abs()\n",
    "            mae_dist += (diff * mb[:,3]).sum().item()\n",
    "            cnt_dist += mb[:,3].sum().item()\n",
    "\n",
    "        # ---- Sum (Stay sorted) ----\n",
    "        ttc_pr   = 1.0 / (out[\"ttc\"].clamp_min(1e-6) + EPS)\n",
    "        dmin_pr  = 1.0 / (out[\"dmin\"].clamp_min(1e-6) + EPS)\n",
    "        ttc_gt   = 1.0 / (yb[:,0].clamp_min(1e-6) + EPS)\n",
    "        dmin_gt  = 1.0 / (yb[:,1].clamp_min(1e-6) + EPS)\n",
    "\n",
    "        yT_gt.extend(ttc_gt.cpu().numpy());  yT_pr.extend(ttc_pr.cpu().numpy());  mT.extend(mb[:,0].cpu().numpy())\n",
    "        yD_gt.extend(dmin_gt.cpu().numpy()); yD_pr.extend(dmin_pr.cpu().numpy()); mD.extend(mb[:,1].cpu().numpy())\n",
    "\n",
    "    # Metric(MAE)\n",
    "    logs = {\n",
    "        \"val_loss\": loss_sum / max(1,n),\n",
    "        \"mae_ttc_s\":   (mae_ttc  / max(1,cnt_ttc)),\n",
    "        \"mae_dmin_m\":  (mae_dmin / max(1,cnt_dmin)),\n",
    "        \"mae_tstar_s\": (mae_tstar/ max(1,cnt_tstar)),\n",
    "        \"mae_dist_m\":  (mae_dist / max(1,cnt_dist)),\n",
    "    }\n",
    "\n",
    "    # Metric of event-driven classification(whole validation)\n",
    "    yT_gt = np.array(yT_gt); yT_pr = np.array(yT_pr); mT = np.array(mT, dtype=bool)\n",
    "    yD_gt = np.array(yD_gt); yD_pr = np.array(yD_pr); mD = np.array(mD, dtype=bool)\n",
    "    common = mT & mD\n",
    "    if common.sum() > 0:\n",
    "        tgt_ttc  = yT_gt[common];  prd_ttc  = yT_pr[common]\n",
    "        tgt_dmin = yD_gt[common];  prd_dmin = yD_pr[common]\n",
    "\n",
    "        risk_gt = ((tgt_ttc < 2.5) | (tgt_dmin < 3.0)).astype(np.uint8)\n",
    "        score   = (1.0/np.clip(prd_ttc,1e-3,None)) + (1.0/np.clip(prd_dmin,1e-3,None))\n",
    "\n",
    "        roc = roc_auc_score(risk_gt, score)\n",
    "        pr  = average_precision_score(risk_gt, score)\n",
    "        prec, rec, thr = precision_recall_curve(risk_gt, score)\n",
    "        f1 = 2*prec*rec/(prec+rec+1e-9)\n",
    "        best_idx = int(np.argmax(f1))\n",
    "        # Since return of precision_recall_curve, thr is shorter than prec/rec by 1\n",
    "        best_thr = float(thr[max(best_idx-1, 0)]) if thr.size>0 else 0.5\n",
    "\n",
    "        logs.update({\n",
    "            \"ev_n\": int(common.sum()),\n",
    "            \"ev_roc_auc\": float(roc),\n",
    "            \"ev_pr_auc\":  float(pr),\n",
    "            \"ev_best_f1\": float(f1[best_idx]),\n",
    "            \"ev_best_thr\": best_thr,\n",
    "            \"ev_best_p\":  float(prec[best_idx]),\n",
    "            \"ev_best_r\":  float(rec[best_idx]),\n",
    "        })\n",
    "    else:\n",
    "        logs.update({\n",
    "            \"ev_n\": 0,\n",
    "            \"ev_roc_auc\": np.nan,\n",
    "            \"ev_pr_auc\":  np.nan,\n",
    "            \"ev_best_f1\": np.nan,\n",
    "            \"ev_best_thr\": np.nan,\n",
    "            \"ev_best_p\":  np.nan,\n",
    "            \"ev_best_r\":  np.nan,\n",
    "        })\n",
    "\n",
    "    return logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bd8f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01] train -0.0638 (ttc 0.0269 dmin 0.0081 t* 0.0676 dist 0.0020)  |  val -0.1490 [MAE: ttc 2.221s, dmin 5.095m, t* 1.343s, dist 8.021m]  | [EVENT: N=428 ROC-AUC 0.617 PR-AUC 0.534 BestF1 0.596 @thr≈0.374 (P 0.545, R 0.657)] (152.8s)\n",
      "  ↳ saved best to best_multitarget.pt (ev_pr_auc=0.5336)\n",
      "[02] train -0.4185 (ttc 0.0204 dmin 0.0069 t* 0.0555 dist 0.0018)  |  val -0.4642 [MAE: ttc 1.838s, dmin 4.585m, t* 1.415s, dist 7.955m]  | [EVENT: N=428 ROC-AUC 0.802 PR-AUC 0.734 BestF1 0.699 @thr≈0.389 (P 0.565, R 0.916)] (153.6s)\n",
      "  ↳ saved best to best_multitarget.pt (ev_pr_auc=0.7343)\n",
      "[03] train -0.7512 (ttc 0.0167 dmin 0.0062 t* 0.0517 dist 0.0017)  |  val -0.7691 [MAE: ttc 1.879s, dmin 4.618m, t* 1.357s, dist 7.486m]  | [EVENT: N=428 ROC-AUC 0.755 PR-AUC 0.701 BestF1 0.716 @thr≈0.381 (P 0.677, R 0.759)] (153.7s)\n",
      "[04] train -1.0674 (ttc 0.0160 dmin 0.0059 t* 0.0478 dist 0.0017)  |  val -1.0304 [MAE: ttc 1.996s, dmin 4.689m, t* 1.335s, dist 8.130m]  | [EVENT: N=428 ROC-AUC 0.774 PR-AUC 0.689 BestF1 0.685 @thr≈0.397 (P 0.585, R 0.825)] (152.7s)\n",
      "[05] train -1.3631 (ttc 0.0129 dmin 0.0053 t* 0.0469 dist 0.0016)  |  val -1.2804 [MAE: ttc 1.937s, dmin 4.506m, t* 1.410s, dist 7.333m]  | [EVENT: N=428 ROC-AUC 0.792 PR-AUC 0.713 BestF1 0.680 @thr≈0.360 (P 0.734, R 0.633)] (150.6s)\n",
      "[unfreeze] unlocked: backbone[7] (layer4)\n",
      "[unfreeze] optimizer/scheduler reset.\n",
      "[06] train -1.5717 (ttc 0.0092 dmin 0.0039 t* 0.0357 dist 0.0015)  |  val -1.3876 [MAE: ttc 1.765s, dmin 4.432m, t* 1.306s, dist 7.956m]  | [EVENT: N=428 ROC-AUC 0.797 PR-AUC 0.715 BestF1 0.668 @thr≈0.409 (P 0.561, R 0.825)] (154.2s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bocchi/workspace/capstone/code/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:209: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07] train -1.6949 (ttc 0.0096 dmin 0.0044 t* 0.0399 dist 0.0016)  |  val -1.5340 [MAE: ttc 1.820s, dmin 4.695m, t* 1.358s, dist 8.125m]  | [EVENT: N=428 ROC-AUC 0.794 PR-AUC 0.714 BestF1 0.688 @thr≈0.445 (P 0.561, R 0.892)] (151.9s)\n",
      "[08] train -1.9612 (ttc 0.0098 dmin 0.0045 t* 0.0388 dist 0.0016)  |  val -1.8479 [MAE: ttc 1.735s, dmin 4.287m, t* 1.307s, dist 7.359m]  | [EVENT: N=428 ROC-AUC 0.850 PR-AUC 0.792 BestF1 0.754 @thr≈0.350 (P 0.733, R 0.777)] (158.0s)\n",
      "  ↳ saved best to best_multitarget.pt (ev_pr_auc=0.7917)\n",
      "[09] train -2.3048 (ttc 0.0107 dmin 0.0042 t* 0.0328 dist 0.0016)  |  val -2.1417 [MAE: ttc 1.928s, dmin 4.626m, t* 1.280s, dist 7.775m]  | [EVENT: N=428 ROC-AUC 0.748 PR-AUC 0.703 BestF1 0.656 @thr≈0.527 (P 0.581, R 0.753)] (152.4s)\n",
      "[10] train -2.6421 (ttc 0.0075 dmin 0.0035 t* 0.0290 dist 0.0016)  |  val -2.4231 [MAE: ttc 1.765s, dmin 4.440m, t* 1.245s, dist 7.579m]  | [EVENT: N=428 ROC-AUC 0.802 PR-AUC 0.771 BestF1 0.671 @thr≈0.666 (P 0.812, R 0.572)] (157.2s)\n",
      "[11] train -2.9316 (ttc 0.0074 dmin 0.0034 t* 0.0266 dist 0.0015)  |  val -2.5589 [MAE: ttc 1.819s, dmin 4.341m, t* 1.323s, dist 7.089m]  | [EVENT: N=428 ROC-AUC 0.803 PR-AUC 0.777 BestF1 0.701 @thr≈0.575 (P 0.805, R 0.620)] (155.7s)\n",
      "[12] train -3.1994 (ttc 0.0073 dmin 0.0030 t* 0.0246 dist 0.0015)  |  val -2.7872 [MAE: ttc 1.774s, dmin 4.056m, t* 1.198s, dist 7.442m]  | [EVENT: N=428 ROC-AUC 0.802 PR-AUC 0.755 BestF1 0.675 @thr≈0.411 (P 0.560, R 0.849)] (158.3s)\n",
      "[13] train -3.4525 (ttc 0.0064 dmin 0.0029 t* 0.0218 dist 0.0015)  |  val -2.9397 [MAE: ttc 1.732s, dmin 4.191m, t* 1.251s, dist 7.629m]  | [EVENT: N=428 ROC-AUC 0.847 PR-AUC 0.805 BestF1 0.723 @thr≈0.522 (P 0.681, R 0.771)] (152.3s)\n",
      "  ↳ saved best to best_multitarget.pt (ev_pr_auc=0.8054)\n",
      "[14] train -3.6858 (ttc 0.0067 dmin 0.0024 t* 0.0192 dist 0.0014)  |  val -3.0610 [MAE: ttc 1.881s, dmin 4.195m, t* 1.230s, dist 7.425m]  | [EVENT: N=428 ROC-AUC 0.815 PR-AUC 0.780 BestF1 0.698 @thr≈0.607 (P 0.723, R 0.675)] (151.5s)\n",
      "[15] train -3.9023 (ttc 0.0053 dmin 0.0021 t* 0.0174 dist 0.0014)  |  val -3.1322 [MAE: ttc 1.826s, dmin 4.246m, t* 1.237s, dist 7.353m]  | [EVENT: N=428 ROC-AUC 0.814 PR-AUC 0.777 BestF1 0.685 @thr≈0.643 (P 0.805, R 0.596)] (151.6s)\n",
      "[16] train -4.1017 (ttc 0.0038 dmin 0.0020 t* 0.0163 dist 0.0013)  |  val -3.1249 [MAE: ttc 1.824s, dmin 4.096m, t* 1.310s, dist 7.548m]  | [EVENT: N=428 ROC-AUC 0.816 PR-AUC 0.762 BestF1 0.695 @thr≈0.485 (P 0.588, R 0.849)] (150.9s)\n",
      "[17] train -4.2640 (ttc 0.0036 dmin 0.0019 t* 0.0147 dist 0.0012)  |  val -3.1913 [MAE: ttc 1.755s, dmin 3.974m, t* 1.268s, dist 7.303m]  | [EVENT: N=428 ROC-AUC 0.797 PR-AUC 0.754 BestF1 0.675 @thr≈0.417 (P 0.568, R 0.831)] (151.2s)\n",
      "[18] train -4.3912 (ttc 0.0037 dmin 0.0017 t* 0.0147 dist 0.0011)  |  val -3.2850 [MAE: ttc 1.841s, dmin 4.119m, t* 1.224s, dist 7.324m]  | [EVENT: N=428 ROC-AUC 0.802 PR-AUC 0.768 BestF1 0.684 @thr≈0.477 (P 0.600, R 0.795)] (151.1s)\n",
      "[19] train -4.5091 (ttc 0.0037 dmin 0.0017 t* 0.0129 dist 0.0012)  |  val -3.3454 [MAE: ttc 1.792s, dmin 4.047m, t* 1.233s, dist 7.468m]  | [EVENT: N=428 ROC-AUC 0.822 PR-AUC 0.774 BestF1 0.690 @thr≈0.511 (P 0.624, R 0.771)] (151.2s)\n",
      "[20] train -4.6088 (ttc 0.0030 dmin 0.0015 t* 0.0119 dist 0.0011)  |  val -3.3698 [MAE: ttc 1.848s, dmin 4.001m, t* 1.214s, dist 7.411m]  | [EVENT: N=428 ROC-AUC 0.811 PR-AUC 0.771 BestF1 0.689 @thr≈0.494 (P 0.601, R 0.807)] (152.9s)\n",
      "Early stopping triggered.\n",
      "done. best ev_pr_auc: 0.8054159524600638\n"
     ]
    }
   ],
   "source": [
    "# === Train loop ===\n",
    "EPOCHS = 25\n",
    "BEST_KEY = \"ev_pr_auc\"\n",
    "mode = 'max'\n",
    "patience = 7\n",
    "pat_cnt = 0\n",
    "ckpt_path = \"best_multitarget.pt\"\n",
    "best_val_ev_thr = None  # Save best F1 threshold when validate\n",
    "\n",
    "def is_better(new, best, mode):\n",
    "    eps = 1e-9\n",
    "    return (new > best + eps) if mode == 'max' else (new < best - eps)\n",
    "\n",
    "best_score = -float('inf') if mode == 'max' else float('inf')\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "\n",
    "    # Unfreeze backbone + Reset optima\n",
    "    if epoch == FREEZE_EPOCHS+1:\n",
    "        unlocked = unfreeze_backbone_stage_by_index(model.backbone, stage_idx=7)\n",
    "        model.backbone.apply(freeze_bn)\n",
    "        print(f\"[unfreeze] unlocked: {unlocked} (layer4)\")\n",
    "        bb_trainable_ids = {id(p) for p in model.backbone.parameters() if p.requires_grad}\n",
    "        backbone_params = []\n",
    "        head_params = []\n",
    "        for _, p in model.named_parameters():\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            (backbone_params if id(p) in bb_trainable_ids else head_params).append(p)\n",
    "        opt = torch.optim.AdamW([\n",
    "            {'params': head_params, 'lr': 3e-4, 'weight_decay': 1e-4},\n",
    "            {'params': backbone_params, 'lr': 3e-5, 'weight_decay': 1e-5}\n",
    "        ], betas=(0.9, 0.999))\n",
    "        from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "        warmup = LinearLR(opt, start_factor=0.2, end_factor=1.0, total_iters=2)\n",
    "        cosine = CosineAnnealingLR(opt, T_max=max(1, EPOCHS - FREEZE_EPOCHS - 2))\n",
    "        sched = SequentialLR(opt, schedulers=[warmup, cosine], milestones=[2])\n",
    "        print(\"[unfreeze] optimizer/scheduler reset.\")\n",
    "\n",
    "    # ---- train one epoch ----\n",
    "    tr_loss = tr_Lttc = tr_Ldmin = tr_Ltstar = tr_Ldist = 0.0\n",
    "    n = 0\n",
    "    for xb, yb, mb in train_dl:\n",
    "        xb, yb, mb = xb.to(device), yb.to(device), mb.to(device)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        out = model(xb)\n",
    "        loss, logs = multitask_loss(out, yb, mb, model)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        opt.step()\n",
    "\n",
    "        bs = xb.size(0)\n",
    "        tr_loss  += loss.item() * bs\n",
    "        tr_Lttc  += logs[\"L_ttc\"]  * bs\n",
    "        tr_Ldmin += logs[\"L_dmin\"] * bs\n",
    "        tr_Ltstar+= logs[\"L_tstar\"]* bs\n",
    "        tr_Ldist += logs[\"L_dist\"] * bs\n",
    "        n += bs\n",
    "\n",
    "    if sched is not None:\n",
    "        sched.step()\n",
    "\n",
    "    # ---- validation ----\n",
    "    val_logs = evaluate_epoch(val_dl)\n",
    "\n",
    "    # ---- logging ----\n",
    "    tr_logs = {\n",
    "        \"train_loss\": tr_loss/max(1,n),\n",
    "        \"L_ttc\": tr_Lttc/max(1,n),\n",
    "        \"L_dmin\": tr_Ldmin/max(1,n),\n",
    "        \"L_tstar\": tr_Ltstar/max(1,n),\n",
    "        \"L_dist\": tr_Ldist/max(1,n),\n",
    "    }\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"[{epoch:02d}] \"\n",
    "          f\"train {tr_logs['train_loss']:.4f} \"\n",
    "          f\"(ttc {tr_logs['L_ttc']:.4f} dmin {tr_logs['L_dmin']:.4f} \"\n",
    "          f\"t* {tr_logs['L_tstar']:.4f} dist {tr_logs['L_dist']:.4f})  |  \"\n",
    "          f\"val {val_logs['val_loss']:.4f} \"\n",
    "          f\"[MAE: ttc {val_logs['mae_ttc_s']:.3f}s, \"\n",
    "          f\"dmin {val_logs['mae_dmin_m']:.3f}m, \"\n",
    "          f\"t* {val_logs['mae_tstar_s']:.3f}s, \"\n",
    "          f\"dist {val_logs['mae_dist_m']:.3f}m]  \"\n",
    "          f\"| [EVENT: N={val_logs['ev_n']} \"\n",
    "          f\"ROC-AUC {val_logs['ev_roc_auc']:.3f} \"\n",
    "          f\"PR-AUC {val_logs['ev_pr_auc']:.3f} \"\n",
    "          f\"BestF1 {val_logs['ev_best_f1']:.3f} @thr≈{val_logs['ev_best_thr']:.3f} \"\n",
    "          f\"(P {val_logs['ev_best_p']:.3f}, R {val_logs['ev_best_r']:.3f})] \"\n",
    "          f\"({elapsed:.1f}s)\")\n",
    "\n",
    "    # ---- checkpoint & early stopping ----\n",
    "    score = val_logs[BEST_KEY]\n",
    "    if is_better(score, best_score, mode):\n",
    "        best_score = score\n",
    "        pat_cnt = 0\n",
    "        best_val_ev_thr = val_logs.get('ev_best_thr', None)\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"opt_state\": opt.state_dict(),\n",
    "            \"best_score\": best_score,\n",
    "            'best_val_ev_thr': best_val_ev_thr,\n",
    "            \"cfg\": {\"T_LEN\": T_LEN, \"IMG_SIZE\": IMG_SIZE}\n",
    "        }, ckpt_path)\n",
    "        print(f\"  ↳ saved best to {ckpt_path} (ev_pr_auc={best_score:.4f})\")\n",
    "    else:\n",
    "        pat_cnt += 1\n",
    "        if pat_cnt >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(\"done. best ev_pr_auc:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcaea315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def evaluate_dataset_event(dataloader, fixed_threshold=None, save_prefix=\"test\"):\n",
    "    model.eval()\n",
    "    yT_gt=[]; yT_pr=[]; mT=[]\n",
    "    yD_gt=[]; yD_pr=[]; mD=[]\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, mb in dataloader:\n",
    "            xb, yb, mb = xb.to(device), yb.to(device), mb.to(device)\n",
    "            out = model(xb)\n",
    "            # 역변환\n",
    "            ttc_pr   = 1.0 / (out[\"ttc\"].clamp_min(1e-4) + EPS)\n",
    "            dmin_pr  = 1.0 / (out[\"dmin\"].clamp_min(1e-4) + EPS)\n",
    "            ttc_gt   = 1.0 / (yb[:,0].clamp_min(1e-4) + EPS)\n",
    "            dmin_gt  = 1.0 / (yb[:,1].clamp_min(1e-4) + EPS)\n",
    "            # 누적\n",
    "            yT_gt.extend(ttc_gt.cpu().numpy());  yT_pr.extend(ttc_pr.cpu().numpy());  mT.extend(mb[:,0].cpu().numpy())\n",
    "            yD_gt.extend(dmin_gt.cpu().numpy()); yD_pr.extend(dmin_pr.cpu().numpy()); mD.extend(mb[:,1].cpu().numpy())\n",
    "\n",
    "    yT_gt = np.array(yT_gt); yT_pr = np.array(yT_pr); mT = np.array(mT, dtype=bool)\n",
    "    yD_gt = np.array(yD_gt); yD_pr = np.array(yD_pr); mD = np.array(mD, dtype=bool)\n",
    "    common = mT & mD\n",
    "    assert common.sum() > 0, \"No common valid TTC & dmin samples in test set.\"\n",
    "\n",
    "    tgt_ttc  = yT_gt[common];  prd_ttc  = yT_pr[common]\n",
    "    tgt_dmin = yD_gt[common];  prd_dmin = yD_pr[common]\n",
    "\n",
    "    risk_gt = ((tgt_ttc < 2.5) | (tgt_dmin < 3.0)).astype(np.uint8)\n",
    "    score   = (1.0/np.clip(prd_ttc,1e-4,None)) + (1.0/np.clip(prd_dmin,1e-4,None))\n",
    "\n",
    "    # 곡선/면적\n",
    "    fpr, tpr, roc_thr = roc_curve(risk_gt, score)\n",
    "    roc_auc = roc_auc_score(risk_gt, score)\n",
    "    prec, rec, pr_thr = precision_recall_curve(risk_gt, score)\n",
    "    pr_auc  = average_precision_score(risk_gt, score)\n",
    "\n",
    "    # 테스트 자체에서의 best-F1 (참고용)\n",
    "    f1 = 2*prec*rec/(prec+rec+1e-9)\n",
    "    best_idx = int(np.argmax(f1))\n",
    "    best_f1  = float(f1[best_idx])\n",
    "    best_thr_test = float(pr_thr[max(best_idx-1,0)]) if pr_thr.size>0 else 0.5\n",
    "\n",
    "    # 최종 리포트용 Confusion Matrix 임계값: 고정 임계값 사용(검증에서 저장한 값)\n",
    "    thr_used = float(fixed_threshold) if fixed_threshold is not None else best_thr_test\n",
    "    y_pred = (score >= thr_used).astype(np.uint8)\n",
    "    cm = confusion_matrix(risk_gt, y_pred, labels=[0,1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    P = tp / max(tp+fp, 1e-9)\n",
    "    R = tp / max(tp+fn, 1e-9)\n",
    "    F1_at_thr = 2*P*R / max(P+R, 1e-9)\n",
    "\n",
    "    # --- 그림 저장 ---\n",
    "    # 1) ROC\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f\"ROC (AUC={roc_auc:.3f})\")\n",
    "    plt.plot([0,1],[0,1],'--')\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_prefix}_roc.png\", dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # 2) PR\n",
    "    plt.figure()\n",
    "    plt.plot(rec, prec, label=f\"PR (AP={pr_auc:.3f})\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.title(\"Precision-Recall Curve\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_prefix}_pr.png\", dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # 3) 점수 히스토그램\n",
    "    plt.figure()\n",
    "    plt.hist(score[risk_gt==0], bins=40, alpha=0.7, label=\"safe\")\n",
    "    plt.hist(score[risk_gt==1], bins=40, alpha=0.7, label=\"risk\")\n",
    "    plt.axvline(thr_used, linestyle=\"--\", label=f\"thr={thr_used:.3f}\")\n",
    "    plt.xlabel(\"risk score\")\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Risk Score Distribution\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_prefix}_score_hist.png\", dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # 4) Confusion Matrix (thr_used)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=[\"safe(0)\",\"risk(1)\"])\n",
    "    fig, ax = plt.subplots()\n",
    "    disp.plot(ax=ax, values_format=\"d\", colorbar=False)\n",
    "    ax.set_title(f\"Confusion Matrix @thr={thr_used:.3f}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_prefix}_cm.png\", dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    print(\"score stats:\",\n",
    "      float(score.min()), float(score.max()),\n",
    "      \"thr_used:\", float(thr_used),\n",
    "      \"pos_rate:\", float((score >= thr_used).mean()))\n",
    "\n",
    "    return {\n",
    "        \"N\": int(common.sum()),\n",
    "        \"roc_auc\": float(roc_auc),\n",
    "        \"pr_auc\": float(pr_auc),\n",
    "        \"best_f1_test\": best_f1,\n",
    "        \"best_thr_test\": best_thr_test,\n",
    "        \"thr_used\": thr_used,\n",
    "        \"cm\": {\"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp)},\n",
    "        \"P_at_thr\": float(P),\n",
    "        \"R_at_thr\": float(R),\n",
    "        \"F1_at_thr\": float(F1_at_thr),\n",
    "        \"figs\": {\n",
    "            \"roc\": f\"{save_prefix}_roc.png\",\n",
    "            \"pr\":  f\"{save_prefix}_pr.png\",\n",
    "            \"hist\":f\"{save_prefix}_score_hist.png\",\n",
    "            \"cm\":  f\"{save_prefix}_cm.png\",\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1544efc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_val_ev_thr from validation: 0.5223032236099243\n",
      "score stats: 0.22154535353183746 1.3180476427078247 thr_used: 0.5223032236099243 pos_rate: 0.46238938053097345\n",
      "{'N': 452, 'roc_auc': 0.8167225511878434, 'pr_auc': 0.7143961705534915, 'best_f1_test': 0.6411149820821911, 'best_thr_test': 0.5537711381912231, 'thr_used': 0.5223032236099243, 'cm': {'tn': 211, 'fp': 110, 'fn': 32, 'tp': 99}, 'P_at_thr': 0.47368421052631576, 'R_at_thr': 0.7557251908396947, 'F1_at_thr': 0.5823529411764706, 'figs': {'roc': 'test_roc.png', 'pr': 'test_pr.png', 'hist': 'test_score_hist.png', 'cm': 'test_cm.png'}}\n"
     ]
    }
   ],
   "source": [
    "ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "model.to(device)\n",
    "best_val_ev_thr = ckpt.get(\"best_val_ev_thr\", None)\n",
    "print(\"best_val_ev_thr from validation:\", best_val_ev_thr)\n",
    "\n",
    "test_report = evaluate_dataset_event(test_dl, fixed_threshold=best_val_ev_thr, save_prefix=\"test\")\n",
    "print(test_report)\n",
    "# => test_roc.png, test_pr.png, test_score_hist.png, test_cm.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4533a88",
   "metadata": {},
   "source": [
    "---\n",
    "## For experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c210c38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL MAE/R2]\n",
      "TTC: MAE=1.896s  R2=-0.013\n",
      "dmin: MAE=4.033m  R2=-0.208\n",
      "t*: MAE=1.248s  R2=0.054\n",
      "dist: MAE=3.819m  R2=0.294\n",
      "[VAL EVENT] N=232  ROC-AUC=0.902  PR-AUC=0.856  Best F1=0.854 at thr≈0.346 (P=0.880, R=0.830)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, roc_auc_score, average_precision_score, precision_recall_curve\n",
    "\n",
    "model.eval()\n",
    "# 원본(정렬 유지) 리스트들\n",
    "yT_gt=[]; yT_pr=[]; mT=[]\n",
    "yD_gt=[]; yD_pr=[]; mD=[]\n",
    "yS_gt=[]; yS_pr=[]; mS=[]\n",
    "yC_gt=[]; yC_pr=[]; mC=[]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb, mb in val_dl:\n",
    "        xb, yb, mb = xb.to(device), yb.to(device), mb.to(device)\n",
    "        out = model(xb)\n",
    "\n",
    "        # 역변환 (예측)\n",
    "        ttc_pr   = 1.0 / (out[\"ttc\"].clamp_min(1e-6) + EPS)\n",
    "        dmin_pr  = 1.0 / (out[\"dmin\"].clamp_min(1e-6) + EPS)\n",
    "        tstar_pr = out[\"tstar\"].clamp(0,1) * H\n",
    "        dist_pr  = 1.0 / (out[\"dist\"].clamp_min(1e-6) + EPS)\n",
    "\n",
    "        # 역변환 (GT)\n",
    "        ttc_gt   = 1.0 / (yb[:,0].clamp_min(1e-6) + EPS)\n",
    "        dmin_gt  = 1.0 / (yb[:,1].clamp_min(1e-6) + EPS)\n",
    "        tstar_gt = yb[:,2].clamp(0,1) * H\n",
    "        dist_gt  = 1.0 / (yb[:,3].clamp_min(1e-6) + EPS)\n",
    "\n",
    "        # 그대로(정렬 유지) 누적 + 마스크도 함께 저장\n",
    "        yT_gt.extend(ttc_gt.cpu().numpy());   yT_pr.extend(ttc_pr.cpu().numpy());   mT.extend(mb[:,0].cpu().numpy())\n",
    "        yD_gt.extend(dmin_gt.cpu().numpy());  yD_pr.extend(dmin_pr.cpu().numpy());  mD.extend(mb[:,1].cpu().numpy())\n",
    "        yS_gt.extend(tstar_gt.cpu().numpy()); yS_pr.extend(tstar_pr.cpu().numpy()); mS.extend(mb[:,2].cpu().numpy())\n",
    "        yC_gt.extend(dist_gt.cpu().numpy());  yC_pr.extend(dist_pr.cpu().numpy());  mC.extend(mb[:,3].cpu().numpy())\n",
    "\n",
    "# 넘파이 변환\n",
    "yT_gt=np.array(yT_gt); yT_pr=np.array(yT_pr); mT=np.array(mT).astype(bool)\n",
    "yD_gt=np.array(yD_gt); yD_pr=np.array(yD_pr); mD=np.array(mD).astype(bool)\n",
    "yS_gt=np.array(yS_gt); yS_pr=np.array(yS_pr); mS=np.array(mS).astype(bool)\n",
    "yC_gt=np.array(yC_gt); yC_pr=np.array(yC_pr); mC=np.array(mC).astype(bool)\n",
    "\n",
    "# --- 1) Per-task MAE/R2 (각 타깃 마스크로 별도 집계) ---\n",
    "def report(name, gt, pr, mask, unit):\n",
    "    if mask.sum()==0:\n",
    "        print(f\"{name}: (no valid samples)\")\n",
    "        return\n",
    "    print(f\"{name}: MAE={mean_absolute_error(gt[mask], pr[mask]):.3f}{unit}  \"\n",
    "          f\"R2={r2_score(gt[mask], pr[mask]):.3f}\")\n",
    "\n",
    "print(\"[VAL MAE/R2]\")\n",
    "report(\"TTC\",  yT_gt, yT_pr, mT, \"s\")\n",
    "report(\"dmin\", yD_gt, yD_pr, mD, \"m\")\n",
    "report(\"t*\",   yS_gt, yS_pr, mS, \"s\")\n",
    "report(\"dist\", yC_gt, yC_pr, mC, \"m\")\n",
    "\n",
    "# --- 2) 이벤트 기반 평가: 공통 마스크 사용 ---\n",
    "# 위험 = (TTC < 2.5) OR (dmin < 3.0)\n",
    "common = mT & mD           # 두 타깃이 동시에 유효한 샘플만\n",
    "if common.sum() == 0:\n",
    "    print(\"[VAL EVENT] skip (no common valid TTC & dmin samples)\")\n",
    "else:\n",
    "    tgt_ttc  = yT_gt[common];  prd_ttc  = yT_pr[common]\n",
    "    tgt_dmin = yD_gt[common];  prd_dmin = yD_pr[common]\n",
    "\n",
    "    risk_gt = ((tgt_ttc < 2.5) | (tgt_dmin < 3.0)).astype(np.uint8)\n",
    "    # 위험 스코어 (클수록 위험): 1/TTC + 1/dmin\n",
    "    score = (1.0/np.clip(prd_ttc,1e-3,None)) + (1.0/np.clip(prd_dmin,1e-3,None))\n",
    "    score = (score - score.min()) / (score.max() - score.min() + 1e-9)\n",
    "\n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve\n",
    "    auc = roc_auc_score(risk_gt, score)\n",
    "    ap  = average_precision_score(risk_gt, score)\n",
    "    prec, rec, thr = precision_recall_curve(risk_gt, score)\n",
    "    f1 = 2*prec*rec/(prec+rec+1e-9)\n",
    "    best = np.argmax(f1)\n",
    "\n",
    "    print(f\"[VAL EVENT] N={common.sum()}  ROC-AUC={auc:.3f}  PR-AUC={ap:.3f}  \"\n",
    "          f\"Best F1={f1[best]:.3f} at thr≈{thr[max(best-1,0)]:.3f} \"\n",
    "          f\"(P={prec[best]:.3f}, R={rec[best]:{'.3f'}})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1d80d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NAIVE REG BASELINE]\n",
      "TTC:  MAE=2.274s  R2=0.000\n",
      "dmin: MAE=4.815m  R2=0.000\n",
      "[NAIVE EVENT BASELINE] ROC-AUC=0.500  PR-AUC=0.457\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, roc_auc_score, average_precision_score, precision_recall_curve\n",
    "\n",
    "# 위에서 만든 yT_gt/yT_pr/... mT/mD 그대로 사용\n",
    "common = mT & mD\n",
    "tgt_ttc  = yT_gt[common];  prd_ttc  = yT_pr[common]\n",
    "tgt_dmin = yD_gt[common];  prd_dmin = yD_pr[common]\n",
    "\n",
    "# --- 회귀 베이스라인: 각 타깃의 검증 평균치 예측 ---\n",
    "mean_ttc  = tgt_ttc.mean()\n",
    "mean_dmin = tgt_dmin.mean()\n",
    "bl_ttc  = np.full_like(tgt_ttc,  mean_ttc)\n",
    "bl_dmin = np.full_like(tgt_dmin, mean_dmin)\n",
    "\n",
    "print(\"[NAIVE REG BASELINE]\")\n",
    "print(f\"TTC:  MAE={mean_absolute_error(tgt_ttc, bl_ttc):.3f}s  R2={r2_score(tgt_ttc, bl_ttc):.3f}\")\n",
    "print(f\"dmin: MAE={mean_absolute_error(tgt_dmin, bl_dmin):.3f}m  R2={r2_score(tgt_dmin, bl_dmin):.3f}\")\n",
    "\n",
    "# --- 이벤트 베이스라인: 단일 상수 스코어(무의미) 대신 안전하게 rule 기반 ---\n",
    "# 위험 GT\n",
    "risk_gt = ((tgt_ttc < 2.5) | (tgt_dmin < 3.0)).astype(np.uint8)\n",
    "# 베이스라인 스코어: 예측 평균치로부터 1/ttc + 1/dmin 계산\n",
    "score_bl = (1.0/np.clip(bl_ttc,1e-3,None)) + (1.0/np.clip(bl_dmin,1e-3,None))\n",
    "score_bl = (score_bl - score_bl.min()) / (score_bl.max() - score_bl.min() + 1e-9)\n",
    "\n",
    "auc_bl = roc_auc_score(risk_gt, score_bl)\n",
    "ap_bl  = average_precision_score(risk_gt, score_bl)\n",
    "print(f\"[NAIVE EVENT BASELINE] ROC-AUC={auc_bl:.3f}  PR-AUC={ap_bl:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "549ac124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tr=pd.read_csv(\"index_train.csv\"); va=pd.read_csv(\"index_val.csv\")\n",
    "te=pd.read_csv(\"index_test.csv\")\n",
    "assert set(tr.clip_id).isdisjoint(va.clip_id) and set(tr.clip_id).isdisjoint(te.clip_id) and set(va.clip_id).isdisjoint(te.clip_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8ea99a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_ttc=1 ratio: 0.8722689075630252\n",
      "mask_dmin=1 ratio: 0.980672268907563\n",
      "mask_ttc=1 ratio: 0.8560885608856088\n",
      "mask_dmin=1 ratio: 0.981549815498155\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"index_train.csv\")\n",
    "print(\"mask_ttc=1 ratio:\", df[\"mask_ttc\"].mean())\n",
    "print(\"mask_dmin=1 ratio:\", df[\"mask_dmin\"].mean())\n",
    "df=pd.read_csv(\"index_val.csv\")\n",
    "print(\"mask_ttc=1 ratio:\", df[\"mask_ttc\"].mean())\n",
    "print(\"mask_dmin=1 ratio:\", df[\"mask_dmin\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27c7ee52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL positive rate: 0.45689655172413796\n"
     ]
    }
   ],
   "source": [
    "# (평가 때 썼던 common 마스크 기준) 위험 비율 확인\n",
    "pos_rate = ((tgt_ttc < 2.5) | (tgt_dmin < 3.0)).mean()\n",
    "print(\"VAL positive rate:\", pos_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b89dddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dup frames: 1116\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "paths = list(chain.from_iterable(df[[f\"f{k}\" for k in range(16)]].values.tolist()))\n",
    "dup = [p for p,c in Counter(paths).items() if c>1]\n",
    "print(\"dup frames:\", len(dup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72f061d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 프레임 수: 4336\n",
      "고유 프레임 수: 1180\n",
      "중복 비율: 0.7278597785977861\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "paths = list(chain.from_iterable(df[[f\"f{k}\" for k in range(16)]].values.tolist()))\n",
    "unique_paths = set(paths)\n",
    "print(\"전체 프레임 수:\", len(paths))\n",
    "print(\"고유 프레임 수:\", len(unique_paths))\n",
    "print(\"중복 비율:\", 1 - len(unique_paths)/len(paths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06944d77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
